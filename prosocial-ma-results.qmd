---
title: "Prosocial evaluation - Results and analysis"
bibliography: bibliography.bib
format: 
  pdf:
    keep-tex: true
---

```{r setup, include=F, echo=F}
library(tidyverse)
library(here)
library(glue)

library(patchwork)
library(ggpubr)
library(ggbeeswarm)

library(MAd)
library(metafor)
library(metaviz)
library(emmeans)

source(here("scripts", "smd_helpers.R"))
source(here("scripts", "plot_helpers.R"))

theme_set(theme_bw() +
            theme(panel.grid = element_blank()))

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)
```

```{r data}
df_data <- read_csv(here("data", "data_meta_new_ppt.csv")) |> 
  mutate(s = as.numeric(s),
         t = as.numeric(t),
         scenarios = factor(scenarios) |> 
           fct_relevel("help_hinder", "fair_unfair", "give_take", "other"),
         measure = factor(measure) |>
           fct_relevel("reach", "help", "visual")) |> 
  rename(scenario = scenarios)

df_data_d <- df_data |> 
  mutate(d_calc = case_when(
    is.na(t) & !is.na(s) ~ calculate_d("proportion", ni = ni, s = s),
    !is.na(t) ~ calculate_d("ttest", ni = ni, t = t),
    .default = calculate_d("success", ni = ni, xi = xi)
  )) |> 
  unnest(cols = c(d_calc)) |> 
  mutate(d_se = sqrt(d_var))
```

# Results

## Average Point Estimate

```{r ma-full}
df_data_agg <- agg(
  id = ID,
  es = d,
  var = d_var,
  method = "BHHR",
  data = df_data_d
) |> 
  mutate(study = df_data_d$study |> unique() |> study_label()) |> 
  rename(d = es,
         d_var = var) |> 
  mutate(d_se = sqrt(d_var))

df_data_agg_trans <- df_data_agg |> 
  mutate(d_ci.lb = d - stats::qnorm(0.975) * d_se,
         d_ci.ub = d + stats::qnorm(0.975) * d_se,
         prop = d_to_prop(d),
         prop_ci.lb = d_to_prop(d_ci.lb),
         prop_ci.ub = d_to_prop(d_ci.ub)) |> 
  left_join(df_data_d |> 
              select(ID, published) |> 
              distinct(),
            by = join_by(id == ID))

ma_agg <- rma.uni(yi = d,
                  vi = d_var,
                  data = df_data_agg)
ma_re <- rma.mv(yi = d,
                V = d_var,
                random = ~ 1 | study/ppt_grp,
                data = df_data_d)
```

```{r fig-forest}
#| fig-cap: "Forest plot depicting paper-level raw proportions and their 95% confidence intervals. Raw proportions were derived by back-transforming the estimated Cohen's _d_ and its variance. The black dotted line at 0.50 indicates the reference point corresponding to no preference or random responding, and the blue dashed line indicates the meta-analytic mean."
#| fig-width: 7
#| fig-height: 9

p_forest_sorted <- df_data_agg_trans |> 
  mutate(prec = 1 / (d_var + ma_agg$tau2),
         weight = prec / sum(prec)) |> 
  arrange(prop) |> 
  mutate(id = row_number()) |>
  make_forest_plot(ma_re)

p_forest_sorted
```

We begin by calculating an average point estimate of the effect size for infants' preference for prosocial over antisocial agents, reporting both Cohen's _d_ and its back-transformation into raw proportions to improve interpretability. The included studies contributed a total of $k =$ `r nrow(df_data)` effect sizes to analyze (dataset available at [OSF](https://osf.io/r4av3/?view_only=aa69cf6c08884058b0cc72a75d55347c)). We used a random effects meta-analysis to account for heterogeneity across studies, with a nested random structure of participant group within paper. We report all estimates with their 95\% confidence intervals.

A forest plot for the meta-analysis is shown in @fig-forest; note that the forest plot displays effect sizes aggregated by paper for concision, but all meta-analyses treated each effect size separately. The estimated meta-analytic effect size was Cohen's $d =$ `r print_est_ci(ma_re$b[1], ma_re$ci.lb, ma_re$ci.ub)`. When back-transformed to raw proportions, this effect size corresponds to `r print_est_ci(d_to_prop(ma_re$b[1]), d_to_prop(ma_re$ci.lb), d_to_prop(ma_re$ci.ub))`. A significant Q-test confirmed the presence of heterogeneity, $Q$(`r ma_re$QEdf`) = `r print_num(ma_re$QE, 2)`, `r print_p(ma_re$QEp)`. The estimated variance across papers was $\sigma^2 =$ `r print_num(ma_re$sigma2[1])`, and the estimate variance across participant groups within papers was $\sigma^2 =$ `r print_num(ma_re$sigma2[2])`. Given this variability between studies, we proceeded to examine potential moderators to identify factors that might account for differences in infants' sociomoral preferences across experiments.

## Publication Bias

```{r ma-pub}
mod_pub <- rma.mv(yi = d,
                  V = d_var,
                  random = ~ 1 | study/ppt_grp,
                  mods = ~ published,
                  data = df_data_d)

p_pub <- make_moderator_plot(df_data_d, mod_pub) +
  labs(x = "Publication status") +
  scale_x_discrete(
    limits = c("yes", "no"),
    labels = c("Published", "Unpublished")
  )
```

```{r eggers}
df_data_pubonly <- df_data_d |> 
  filter(published == "yes")

mod_egger_pubonly <- rma.mv(yi = d,
                            V = d_var,
                            random = ~ 1 | study/ppt_grp,
                            mods = ~ d_se,
                            data = df_data_pubonly)
```

```{r fig-funnel}
#| fig-cap: "Funnel plot depicting standard errors as a function of effect sizes. The dashed vertical line represents the meta-analytic mean. The outer dotted lines delineate the triangular region within which 95% of effect sizes are expected to fall if there is no publication bias and true effect sizes are homogeneous."
#| fig-width: 6
#| fig-height: 4

summary_es <- ma_re$b[1,1]
summary_tau2 <- 0 # ma_re$tau2
summary_se <- ma_re$se
max_se <- max(df_data_d$d_se) + max(df_data_d$d_se) * 0.2

funneldata <- tibble(
  x = c(summary_es - stats::qnorm(0.975) * sqrt(max_se^2 + summary_tau2),
        summary_es - stats::qnorm(0.975) * sqrt(summary_tau2),
        summary_es + stats::qnorm(0.975) * sqrt(summary_tau2),
        summary_es + stats::qnorm(0.975) * sqrt(max_se^2 + summary_tau2)),
  y = c(max_se, 0, 0, max_se)
)

funneldata_dense <- funneldata |> 
  mutate(line_id = c(0, 0, 1, 1)) |>
  nest(data = -line_id) |> 
  mutate(interp = map(data, 
                      \(d) approx(d$x, d$y, n = 200) |> as_tibble())) |> 
  dplyr::select(-data) |> 
  unnest(interp)

p_funnel <- df_data_d |> 
  ggplot(aes(x = d, y = d_se)) +
  geom_path(data = funneldata, 
            aes(x = x, y = y),
            lty = "dashed",
            col = "gray27") +
  geom_path(data = tibble(
    x = c(summary_es, summary_es),
    y = c(0, max_se)), 
    aes(x = x, y = y), 
    lty = "dashed",
    col = "gray27") +
  geom_point(
    aes(shape = published),
    col = PLOT_COL,
    alpha = .7,
    size = 2) +
  scale_y_reverse() +
  PUB_SCALE +
  coord_cartesian(ylim = c(max_se * 0.95, 0)) +
  labs(x = "Cohen's d",
       y = "Standard error") +
  theme(legend.position = "bottom")

p_funnel
```

An initial analysis examined _publication status_ (published vs. unpublished) as a potential moderator of effect size. @fig-funnel displays a funnel plot of the effect sizes. No significant moderation was observed, `r print_qm(mod_pub)`. We assessed evidence of publication bias (@Ferguson2012) using the $k =$ `r df_data_d |> filter(published == "yes") |> nrow()` published effect sizes. We used a generalisation of Egger's test (@Egger1997), regressing the effect sizes from their standard errors; publication bias was indicated if the intercept was significantly different than zero. We did not find presence of publication bias, with the intercept estimated as `r print_est_ci(mod_egger_pubonly$b[1], mod_egger_pubonly$ci.lb[1], mod_egger_pubonly$ci.ub[1])`, `r print_p(mod_egger_pubonly$pval[1])`. 

## Age and Sample Size

```{r age}
df_data_d <- df_data_d |> 
  mutate(prec = 1 / (d_var + ma_re$tau2),
         weight = prec / sum(prec))

mod_age <- rma.mv(yi = d,
                  V = d_var,
                  random = ~ 1 | study/ppt_grp,
                  mods = ~ mean_age_days,
                  data = df_data_d)
pred_age <- predict(mod_age)

df_age_pred <- df_data_d |> 
  mutate(d_pred = pred_age$pred,
         d_pred_ci.lb = pred_age$ci.lb,
         d_pred_ci.ub = pred_age$ci.ub,
         prop_pred = d_to_prop(d_pred),
         prop_pred_ci.lb = d_to_prop(d_pred_ci.lb),
         prop_pred_ci.ub = d_to_prop(d_pred_ci.ub))

p_age <- df_data_d |> 
  mutate(prop = d_to_prop(d)) |> 
  ggplot(aes(x = mean_age_days, y = prop)) +
  geom_point(aes(size = weight, shape = published), 
             col = PLOT_COL, alpha = .7) +
  geom_ribbon(data = df_age_pred,
              aes(x = mean_age_days,
                  y = prop_pred,
                  ymin = prop_pred_ci.lb,
                  ymax = prop_pred_ci.ub),
              fill = PLOT_COL,
              alpha = .2) +
  geom_line(data = df_age_pred,
            aes(x = mean_age_days,
                y = prop_pred),
            col = PLOT_COL,
            linewidth = 1) +
  scale_size_continuous(range = c(1, 3), 
                        guide = "none") +
  PUB_SCALE +
  labs(x = "Mean age (days)",
       y = "Proportion preferring prosocial agent")
```

```{r sample-size}
mod_n <- rma.mv(yi = d,
                V = d_var,
                random = ~ 1 | study/ppt_grp,
                mods = ~ log(ni),
                data = df_data_d)
pred_n <- predict(mod_n)

df_data_d_noout <- df_data_d |>
  mutate(log_n = log(ni),
         log_n_z = scale(log_n)[,1]) |> 
  filter(log_n_z >= -3,
         log_n_z <= 3)

mod_n_noout <- rma.mv(yi = d,
                        V = d_var,
                        random = ~ 1 | study/ppt_grp,
                        mods = ~ log(ni),
                        data = df_data_d_noout)

df_n_pred <- df_data_d |> 
  mutate(d_pred = pred_n$pred,
         d_pred_ci.lb = pred_n$ci.lb,
         d_pred_ci.ub = pred_n$ci.ub,
         prop_pred = d_to_prop(d_pred),
         prop_pred_ci.lb = d_to_prop(d_pred_ci.lb),
         prop_pred_ci.ub = d_to_prop(d_pred_ci.ub))

p_n <- df_data_d |> 
  mutate(prop = d_to_prop(d)) |> 
  ggplot(aes(x = log(ni), y = prop)) +
  geom_point(aes(size = weight, shape = published), 
             col = PLOT_COL, alpha = .7) +
  geom_ribbon(data = df_n_pred,
              aes(x = log(ni),
                  y = prop_pred,
                  ymin = prop_pred_ci.lb,
                  ymax = prop_pred_ci.ub),
              fill = PLOT_COL,
              alpha = .2) +
  geom_line(data = df_n_pred,
            aes(x = log(ni),
                y = prop_pred),
            col = PLOT_COL,
            linewidth = 1) +
  annotate("text",
           x = log(442) / 2,
           y = 1.05,
           label = "*") +
  scale_size_continuous(range = c(1, 3), 
                        guide = "none") +
  PUB_SCALE +
  labs(x = "Log sample size",
       y = "Proportion preferring prosocial agent")
```

```{r scenario}
mod_scenario <- rma.mv(yi = d,
                       V = d_var,
                       random = ~ 1 | study/ppt_grp,
                       mods = ~ factor(scenario),
                       data = df_data_d)

p_scenario <- make_moderator_plot(df_data_d, mod_scenario)

emm_scenario <- emmprep(mod_scenario) |> emmeans( ~ scenario) |> as_tibble()
```

```{r measure}
mod_measure <- rma.mv(yi = d,
                      V = d_var,
                      random = ~ 1 | study/ppt_grp,
                      mods = ~ factor(measure),
                      data = df_data_d)

p_measure <- make_moderator_plot(df_data_d, mod_measure)

emm_measure <- emmprep(mod_measure) |> emmeans( ~ measure) |> as_tibble()
```

```{r fig-moderators}
#| fig-cap: "Effect sizes shown as a function of (A) infants' mean age in days, (B) log sample size, (C) scenario type, and (D) the measure used to assess preferences. For clarity of interpretation, effect sizes are presented in raw proportions. Lines and shaded areas represent the predicted meta-regression effects and their 95% confidence intervals. Diamonds represent estimated marginal means. *: $p$ < .05."
#| fig-width: 7
#| fig-height: 7

p_moderators <- (p_age + p_n + p_scenario + p_measure) +
  # plot_layout(axis_titles = "collect") +
  plot_layout(guides = "collect") +
  plot_annotation(tag_levels = 'A') &
  scale_y_continuous(limits = c(0, 1.1),
                     breaks = seq(0, 1, 0.25)) &
  theme(legend.position = "bottom")

p_moderators
```

We first investigated the possible effect of key sample-related moderators, including age and sample size. The mean ages of the samples ranged between `r df_data_d$mean_age_days |> min()` days and `r df_data_d$mean_age_days |> max()` days. As seen in [@fig-moderators]A, _age_ did not significantly moderate the effect size, `r print_qm(mod_age)`, with the estimated effect of one additional month of age being $\Delta d =$ `r print_est_ci(30*mod_age$b[2], 30*mod_age$ci.lb[2], 30*mod_age$ci.ub[2])`. 

[@fig-moderators]B shows the effect of log _sample size_, which was a significant moderator of effect size, `r print_qm(mod_n)`. Studies with larger samples reported smaller effects. Examination of the distribution of sample size suggested that there were a few outliers; thus, we reran the analysis excluding effect sizes with log sample sizes beyond 3 standard deviations from the mean. Five effect sizes were excluded: one from @Lucca2025 ($n = 1$), and four from @Hamlin2025 ($n \in [214, 442]$). After their exclusion, log sample size remained a significant moderator of effect size, `r print_qm(mod_n_noout)`. The estimated effect of one additional participant was $\Delta d =$ `r print_est_ci(mod_n_noout$b[2], mod_n_noout$ci.lb[2], mod_n_noout$ci.ub[2])`.

## Scenario Type and Measure Type
We next investigated two key design-related moderators, namely scenario type and measure type. _Scenario type_ (four levels: fair/unfair, give/take, help/hinder, other) did not significantly moderate the effect size, `r print_qm(mod_scenario)`, as shown in [@fig-moderators]C. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_scenario |> filter(scenario == "fair_unfair"))` in fair/unfair scenarios, `r print_emm(emm_scenario |> filter(scenario == "give_take"))` in give/take scenarios, `r print_emm(emm_scenario |> filter(scenario == "help_hinder"))` in help/hinder scenarios, and `r print_emm(emm_scenario |> filter(scenario == "other"))` in other scenarios.

_Measure type_ (three levels: reach, help, visual) did not emerge as a significant moderator of effect size, `r print_qm(mod_measure)`, as shown in [@fig-moderators]D. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_measure |> filter(measure == "reach"))` in studies employing a manual reaching measure, `r print_emm(emm_measure |> filter(measure == "help"))` in studies using selective helping as the outcome, and `r print_emm(emm_measure |> filter(measure == "visual"))` in studies assessing visual preference.

## Laboratory Bias and Study Location

```{r lab}
mod_lab <- rma.mv(yi = d,
                     V = d_var,
                     random = ~ 1 | study/ppt_grp,
                     mods = ~ H_Lab_narrow,
                     data = df_data_d)

p_lab <- make_moderator_plot(df_data_d, mod_lab) +
  labs(x = "lab lab") +
  scale_x_discrete(
    limits = c("yes", "no"),
    labels = str_to_sentence
  )

emm_lab <- emmprep(mod_lab) |> emmeans( ~ H_Lab_narrow) |> as_tibble()
```

```{r lab_wide}
df_data_lab_wide <- df_data_d |> 
  mutate(H_Lab_wide = ifelse(study %in% c("Woo_Spelke_2023a", "Woo_Spelke_2023b"), 
                             "yes", H_Lab_narrow))
mod_lab_wide <- rma.mv(yi = d,
                     V = d_var,
                     random = ~ 1 | study/ppt_grp,
                     mods = ~ H_Lab_wide,
                     data = df_data_lab_wide)

emm_lab_wide <- emmprep(mod_lab_wide) |> emmeans( ~ H_Lab_wide) |> as_tibble()
```

```{r country}
mod_country <- rma.mv(yi = d,
                        V = d_var,
                        random = ~ 1 | study/ppt_grp,
                        mods = ~ nationality,
                        data = df_data_d |> filter(!is.na(nationality)))

p_country <- make_moderator_plot(
  df_data_d |> filter(!is.na(nationality)),
  mod_country) +
  labs(x = "Country") +
  scale_x_discrete(
    limits = c("North_America", "Europe", "Asia", "Oceania"),
    labels = \(x) x |> str_replace_all("_", " ") |> str_to_title()
  )

emm_country <- emmprep(mod_country) |> emmeans( ~ nationality) |> as_tibble()
```

```{r delivery}
mod_delivery <- rma.mv(yi = d,
                           V = d_var,
                           random = ~ 1 | study/ppt_grp,
                           mods = ~ stimuli,
                           data = df_data_d)

p_delivery <- make_moderator_plot(df_data_d, mod_delivery) +
  labs(x = "Stimuli delivery mode")

emm_delivery <- emmprep(mod_delivery) |> emmeans( ~ stimuli) |> as_tibble()
```

```{r format}
mod_format <- rma.mv(yi = d,
                     V = d_var,
                     random = ~ 1 | study/ppt_grp,
                     mods = ~ stimuli_2,
                     data = df_data_d)

p_format <- make_moderator_plot(df_data_d, mod_format) +
  labs(x = "Presentation format") +
  scale_x_discrete(
    limits = c("real", "cartoon"),
    labels = str_to_sentence
  )

emm_format <- emmprep(mod_format) |> emmeans( ~ stimuli_2) |> as_tibble()
```

```{r agent}
mod_object <- rma.mv(yi = d,
                     V = d_var,
                     random = ~ 1 | study/ppt_grp,
                     mods = ~ choice_object,
                     data = df_data_d)

p_object <- make_moderator_plot(df_data_d, mod_object) +
  labs(x = "Choice object type") +
  scale_x_discrete(
    limits = c("shapes", "puppets", "experimenters"),
    labels = str_to_sentence
  )

emm_object <- emmprep(mod_object) |> emmeans( ~ choice_object) |> as_tibble()
```

```{r prompt}
mod_question <- rma.mv(yi = d,
                       V = d_var,
                       random = ~ 1 | study/ppt_grp,
                       mods = ~ question,
                       data = df_data_d |> 
                         filter(!is.na(question)))

p_question <- make_moderator_plot(
  df_data_d |> filter(!is.na(question)),
  mod_question) +
  scale_x_discrete(
    limits = c("Who do you like", "Other", "No"),
    labels = c("Who do you like?", "Other question", "No question")
  ) +
  labs(x = "Question type")

emm_question <- emmprep(mod_question) |> emmeans( ~ question) |> as_tibble()
```

```{r fig-additional}
#| fig-cap: "Effect sizes shown as a function of (A) laboratory affiliation, (B) study location, (C) stimuli delivery mode, (D) presentation format, (E) agent type, and (F) prompt type. For clarity of interpretation, effect sizes are presented in raw proportions. Diamonds represent estimated marginal means. *: $p$ < .05."
#| fig-width: 7
#| fig-height: 10.5

p_additional <- (p_lab + p_country + p_delivery + p_format + p_object + p_question) +
  # plot_layout(axis_titles = "collect") +
  plot_layout(ncol = 2, 
              guides = "collect") +
  plot_annotation(tag_levels = 'A') &
  scale_y_continuous(limits = c(0, 1.1),
                     breaks = seq(0, 1, 0.25)) &
  theme(legend.position = "bottom")

p_additional
```

We next investigated two potential moderators related to study provenance, namely laboratory affiliation and study location.

We next investigated _laboratory affiliation_ (Hamlin's group versus independent labs) as a potential moderator. Because K. Hamlin pioneered this line of research and contributed a large share of the available data, and -- importantly -- given several failed replication attempts [e.g., @Salvadori2015; @Schlingloff2020], including a recent multi-lab effort [@Lucca2025], we examined whether effect sizes from Hamlin's lab differed systematically from those reported by other groups. In this analysis, the lab-level effects reported in @Lucca2025 were considered independent from Hamlin's laboratory, except for the data directly provided by Hamlin's group. While the multi-lab replication was partly coordinated by Hamlin and collaborators, each participating site independently contributed its own sample, personnel, and recruitment procedures, thereby warranting treatment as distinct sources of data. No significant moderation was detected, `r print_qm(mod_lab)`, as shown in [@fig-additional]A. The estimated proportion of children preferring the prosocial agent in Hamlin's group was `r print_emm(emm_lab |> filter(H_Lab_narrow == "yes"))`, whereas in independent labs it was `r print_emm(emm_lab |> filter(H_Lab_narrow == "no"))`.

As an exploratory extension, we also considered an alternative grouping that included effects reported by B. Woo [specifically, @WooSpelke2023a; @WooSpelke2023b] as part of Hamlin's group, as Woo is a former student of K. Hamlin. This approach is grounded in the concept of "research allegiance" [@Munder2013], since former students may follow similar experimental protocols and methodological guidance in their own research [see for example @Davis2021]. In this analysis, the moderation effect remained insignificant, `r print_qm(mod_lab_wide)`. The estimated proportion of children preferring the prosocial agent in this broader definition of Hamlin's group was `r print_emm(emm_lab_wide |> filter(H_Lab_wide == "yes"))`, whereas in independent labs it was `r print_emm(emm_lab_wide |> filter(H_Lab_wide == "no"))`.

We further examined whether _study location_ (four levels: North America, Europe, Asia, Oceania) moderated the effect size. For this analysis, we dropped one effect size that was collected in South America. Location was not a significant moderator, `r print_qm(mod_country)`, as shown in [@fig-additional]B. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_country |> filter(nationality == "North_America"))` in North America, `r print_emm(emm_country |> filter(nationality == "Europe"))` in Europe, `r print_emm(emm_country |> filter(nationality == "Asia"))` in Asia, and `r print_emm(emm_country |> filter(nationality == "Oceania"))` in Oceania.

## Delivery Mode and Presentation Format

We next investigated two potential moderators related to stimuli presentation. We investigated the effect of _scenario delivery mode_ (live shows vs. movies), revealing a significant moderation effect, `r print_qm(mod_delivery)`, as shown in [@fig-additional]C. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_delivery |> filter(stimuli == "live_show"))` in live shows and `r print_emm(emm_delivery |> filter(stimuli == "movies"))` in movies.

Finally, we examined _scenario presentation format_ (real events vs. cartoons). This moderator was not significant, `r print_qm(mod_format)`, as shown in [@fig-additional]D. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_format |> filter(stimuli_2 == "real"))` in real events and `r print_emm(emm_format |> filter(stimuli_2 == "cartoon"))` in cartoons.

## Agent Type and Prompt Type

Finally, we investigated two potential moderators related to procedure design. _Agent type_ (three levels: experimenters, puppets, foam shapes) did not significantly moderate the effect size, `r print_qm(mod_object)`, as shown in [@fig-additional]E. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_object |> filter(choice_object == "experimenters"))` in studies in which infants and toddlers expressed a preference between two experimenters, `r print_emm(emm_object |> filter(choice_object == "puppets"))` in studies using puppets as agents, and `r print_emm(emm_object |> filter(choice_object == "shapes"))` in studies using shapes.

Last, _prompt type_ (three levels: question "Who do you like?", no prompt, other instructions) did not significantly moderate the effect size, `r print_qm(mod_question)`, as shown in [@fig-additional]F. The estimated proportion of children preferring the prosocial agent was `r print_emm(emm_question |> filter(question == "Who do you like"))` in studies that explicitly asked the question "Who do you like?", `r print_emm(emm_question |> filter(question == "No"))` in studies employing no prompt, and `r print_emm(emm_question |> filter(question == "Other"))` in studies using alternative question wordings or instructions.


